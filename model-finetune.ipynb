{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a67df9",
   "metadata": {},
   "source": [
    "# Fine-Tuning Workshop: Training a Custom AI Assistant for Axiomcart\n",
    "\n",
    "In this workshop, we'll learn how to fine-tune a language model to create a custom AI assistant for customer support. We'll be using the Qwen2-0.5B model and fine-tuning it with LoRA (Low-Rank Adaptation) to answer Axiomcart-specific questions.\n",
    "\n",
    "## What you'll learn:\n",
    "- How to load and use a pre-trained language model\n",
    "- How to prepare training data for fine-tuning\n",
    "- How to implement LoRA for efficient fine-tuning\n",
    "- How to compare base model vs fine-tuned model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39d85f",
   "metadata": {},
   "source": [
    "## Prerequisites:\n",
    "1. Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15348d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==1.9.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs==2.6.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiohttp==3.12.14 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (3.12.14)\n",
      "Requirement already satisfied: aiosignal==1.4.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: asttokens==3.0.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: async-timeout==5.0.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (5.0.1)\n",
      "Requirement already satisfied: attrs==25.3.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (25.3.0)\n",
      "Requirement already satisfied: certifi==2025.7.14 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (2025.7.14)\n",
      "Requirement already satisfied: charset-normalizer==3.4.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (3.4.2)\n",
      "Requirement already satisfied: colorama==0.4.6 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: comm==0.2.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (0.2.2)\n",
      "Requirement already satisfied: contourpy==1.3.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (1.3.2)\n",
      "Requirement already satisfied: cycler==0.12.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: datasets==4.0.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 14)) (4.0.0)\n",
      "Requirement already satisfied: debugpy==1.8.15 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (1.8.15)\n",
      "Requirement already satisfied: decorator==5.2.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 16)) (5.2.1)\n",
      "Requirement already satisfied: dill==0.3.8 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 17)) (0.3.8)\n",
      "Requirement already satisfied: exceptiongroup==1.3.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 18)) (1.3.0)\n",
      "Requirement already satisfied: executing==2.2.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 19)) (2.2.0)\n",
      "Requirement already satisfied: filelock==3.18.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 20)) (3.18.0)\n",
      "Requirement already satisfied: fonttools==4.59.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 21)) (4.59.0)\n",
      "Requirement already satisfied: frozenlist==1.7.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 22)) (1.7.0)\n",
      "Requirement already satisfied: fsspec==2025.3.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 23)) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub==0.33.5 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 24)) (0.33.5)\n",
      "Requirement already satisfied: idna==3.10 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 25)) (3.10)\n",
      "Requirement already satisfied: ipykernel==6.30.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 26)) (6.30.0)\n",
      "Requirement already satisfied: ipython==8.37.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 27)) (8.37.0)\n",
      "Requirement already satisfied: jedi==0.19.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 28)) (0.19.2)\n",
      "Requirement already satisfied: Jinja2==3.1.6 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 29)) (3.1.6)\n",
      "Requirement already satisfied: joblib==1.5.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 30)) (1.5.1)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 31)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.8.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 32)) (5.8.1)\n",
      "Requirement already satisfied: kiwisolver==1.4.8 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 33)) (1.4.8)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 34)) (3.0.2)\n",
      "Requirement already satisfied: matplotlib==3.10.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 35)) (3.10.3)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 36)) (0.1.7)\n",
      "Requirement already satisfied: mpmath==1.3.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 37)) (1.3.0)\n",
      "Requirement already satisfied: multidict==6.6.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 38)) (6.6.3)\n",
      "Requirement already satisfied: multiprocess==0.70.16 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 39)) (0.70.16)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 40)) (1.6.0)\n",
      "Requirement already satisfied: networkx==3.4.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 41)) (3.4.2)\n",
      "Requirement already satisfied: numpy==2.2.6 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 42)) (2.2.6)\n",
      "Requirement already satisfied: packaging==25.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 43)) (25.0)\n",
      "Requirement already satisfied: pandas==2.3.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 44)) (2.3.1)\n",
      "Requirement already satisfied: parso==0.8.4 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 45)) (0.8.4)\n",
      "Requirement already satisfied: peft==0.16.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 46)) (0.16.0)\n",
      "Requirement already satisfied: pillow==11.3.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 47)) (11.3.0)\n",
      "Requirement already satisfied: platformdirs==4.3.8 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 48)) (4.3.8)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.51 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 49)) (3.0.51)\n",
      "Requirement already satisfied: propcache==0.3.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 50)) (0.3.2)\n",
      "Requirement already satisfied: psutil==7.0.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 51)) (7.0.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 52)) (0.2.3)\n",
      "Requirement already satisfied: pyarrow==21.0.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 53)) (21.0.0)\n",
      "Requirement already satisfied: Pygments==2.19.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 54)) (2.19.2)\n",
      "Requirement already satisfied: pyparsing==3.2.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 55)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 56)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2025.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 57)) (2025.2)\n",
      "Requirement already satisfied: pywin32==311 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 58)) (311)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 59)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==27.0.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 60)) (27.0.0)\n",
      "Requirement already satisfied: regex==2024.11.6 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 61)) (2024.11.6)\n",
      "Requirement already satisfied: requests==2.32.4 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 62)) (2.32.4)\n",
      "Requirement already satisfied: safetensors==0.5.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 63)) (0.5.3)\n",
      "Requirement already satisfied: scikit-learn==1.7.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 64)) (1.7.1)\n",
      "Requirement already satisfied: scipy==1.15.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 65)) (1.15.3)\n",
      "Requirement already satisfied: six==1.17.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 66)) (1.17.0)\n",
      "Requirement already satisfied: stack-data==0.6.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 67)) (0.6.3)\n",
      "Requirement already satisfied: sympy==1.14.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 68)) (1.14.0)\n",
      "Requirement already satisfied: threadpoolctl==3.6.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 69)) (3.6.0)\n",
      "Requirement already satisfied: tokenizers==0.21.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 70)) (0.21.2)\n",
      "Requirement already satisfied: torch==2.7.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 71)) (2.7.1)\n",
      "Requirement already satisfied: tornado==6.5.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 72)) (6.5.1)\n",
      "Requirement already satisfied: tqdm==4.67.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 73)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 74)) (5.14.3)\n",
      "Requirement already satisfied: transformers==4.53.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 75)) (4.53.3)\n",
      "Requirement already satisfied: trl==0.19.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 76)) (0.19.1)\n",
      "Requirement already satisfied: typing_extensions==4.14.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 77)) (4.14.1)\n",
      "Requirement already satisfied: tzdata==2025.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 78)) (2025.2)\n",
      "Requirement already satisfied: urllib3==2.5.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 79)) (2.5.0)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 80)) (0.2.13)\n",
      "Requirement already satisfied: xxhash==3.5.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 81)) (3.5.0)\n",
      "Requirement already satisfied: yarl==1.20.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 82)) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de398fc",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de398fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "q:\\src\\model-finetuning-workshop\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Ensure CPU usage (you can change this to \"cuda\" if you have a GPU)\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Suppress warnings which can be ignored to reduce noise in the output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e73567",
   "metadata": {},
   "source": [
    "## Step 2: Load the Base Model and Tokenizer\n",
    "\n",
    "We'll use the Qwen2-0.5B model, which is a compact yet powerful language model suitable for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3740e064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen2-0.5B\n",
      "Model loaded successfully!\n",
      "Model parameters: 494,032,768\n"
     ]
    }
   ],
   "source": [
    "# Load the Qwen 0.5B model and tokenizer\n",
    "model_name = \"Qwen/Qwen2-0.5B\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\").to(device)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {base_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fd6f5",
   "metadata": {},
   "source": [
    "## Step 3: Test Base Model Performance\n",
    "\n",
    "Before fine-tuning, let's see how the base model performs on our Axiomcart-specific questions. \n",
    "This will help us understand the improvement after fine-tuning.\n",
    "\n",
    "We provide Knowledge base in the prompt along with the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fde33404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can I use MasterCard for payment?\n",
      "Response: Axiomcart, a trusted e-commerce platform, strives to deliver orders on time, and we follow several best practices to ensure that our customers receive their orders promptly. Here are some of the measures we take to meet the time frame expectations:\n",
      "\n",
      "1. **Tracking Orders:** To ensure that your order is received on time, we provide you with a detailed tracking number via email notification. This notification can be used to monitor your package's journey in real-time either through our comprehensive order tracking system on our website or by visiting the carrier's official tracking portal for the most up-to-date delivery information.\n",
      "\n",
      "2. **Shipping Options:** We offer a variety of\n",
      "--------------------------------------------------------------------------------\n",
      "Question: How do I return an item?\n",
      "Response: Yes, you can return your purchased items without any hassle by following these simple steps:\n",
      "\n",
      "                    ‚Ä¢ Check if you are eligible for a return. This usually involves following the specific return instructions on the product page. If you are not eligible, you may still receive a full refund.\n",
      "\n",
      "                    ‚Ä¢ Once you have completed the return process, please wait for your refund to be processed.\n",
      "\n",
      "                    ‚Ä¢ If your refund is for a product that was damaged or not as described, you may be eligible for a full refund. Please contact our customer service team for more information.\n",
      "\n",
      "                    ‚Ä¢ If your return is for a product that was damaged or not as described, we will reimburse you for the full amount of the purchase, minus any applicable taxes or shipping fees. Please note that we do not provide refunds for items that are not eligible for a refund, such as items with damages that cannot be repaired or replaced.\n",
      "\n",
      "                    Note: In some cases, you may be required to provide specific evidence of damage or the product not as described\n",
      "--------------------------------------------------------------------------------\n",
      "Question: Do you ship internationally?\n",
      "Response: Yes, we can. However, we do need to\n",
      "--------------------------------------------------------------------------------\n",
      "Question: Is there a discount for new customers?\n",
      "Response: Here is a step-by-step guide to how to get the discount:\n",
      "\n",
      "1. Go to our website and click on the \"My account\" link located on the top right corner.\n",
      "2. In the \"My account\" section, click on the \"Promotions\" link.\n",
      "3. From the promotions section, click on \"First-time Customer Discount\".\n",
      "4. Enter your promo code and click on \"Apply\".\n",
      "5. You will be redirected to a new page where you will see your discount amount and the date of the promotion.\n",
      "6. Use the promo code 'FIRST10' to apply the discount to your first purchase.\n",
      "7. Once the promotion is applied, your first purchase will receive a 10% discount.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "KNOWLEDGE_BASE = \"\"\"\n",
    "    **COMPANY KNOWLEDGE BASE:**\n",
    "\n",
    "    **Account Management:**\n",
    "    - Account Creation: Customers can create accounts by navigating to our comprehensive sign-up page where they will need to carefully fill in all their personal details including their full name, valid email address, and a secure password that meets our security requirements. After completing the registration form and submitting all required information, customers must verify their email address by clicking the verification link sent to their email inbox to fully activate their account and gain access to all platform features.\n",
    "\n",
    "    **Payment Methods:**\n",
    "    - Accepted payments: Axiomcart proudly accepts a wide variety of payment methods to ensure maximum convenience for our valued customers, including all major credit cards such as Visa, MasterCard, and American Express, as well as popular digital payment solutions like PayPal, and traditional bank transfer options for those who prefer direct banking transactions.\n",
    "\n",
    "    **Order Management:**\n",
    "    - Order Tracking: Once your order has been carefully processed by our fulfillment team and handed over to our trusted shipping partners, you will automatically receive a detailed tracking number via email notification. This tracking number can be used to monitor your package's journey in real-time either through our comprehensive order tracking system on our website or by visiting the carrier's official tracking portal for the most up-to-date delivery information.\n",
    "    - Order Changes/Cancellations: Customers have the flexibility to cancel or modify their orders within a 24-hour window from the time of initial placement, provided that the order has not yet been processed by our fulfillment center and moved to the shipping preparation stage. Once an order has entered the processing phase, customers will need to contact our dedicated customer service team who will do their best to accommodate any changes or cancellation requests.\n",
    "\n",
    "    **Returns & Exchanges:**\n",
    "    - Return Policy: Axiomcart maintains a customer-friendly 30-day return policy that allows customers to return items that are in their original, unused condition with all original tags and packaging intact. To initiate a return, customers must first contact our customer service team to obtain proper return authorization and receive detailed instructions on the return process.\n",
    "\n",
    "    **Security & Privacy:**\n",
    "    - Data Protection: At Axiomcart, we take your privacy and data security extremely seriously. We employ industry-standard encryption technologies and robust security protocols to safeguard all personal information provided by our customers. We maintain strict policies regarding data sharing and absolutely do not share, sell, or distribute customer data to any third parties without explicit customer consent, except where required by law.\n",
    "\n",
    "    **Shipping:**\n",
    "    - International Shipping: Axiomcart is proud to offer comprehensive international shipping services to customers in over 50 countries worldwide. Please note that shipping rates, delivery timeframes, and available shipping options may vary significantly depending on your specific geographic location, local customs requirements, and the size and weight of your order.\n",
    "\n",
    "    **Customer Support:**\n",
    "    - Contact Methods: Our dedicated customer support team is available to assist you through multiple convenient channels including direct email communication at support@axiomcart.com, or through our real-time live chat feature readily accessible on our website for immediate assistance.\n",
    "    - Issue Resolution: If you encounter any problems or concerns regarding your order, please don't hesitate to contact our customer service team with your complete order number and a detailed description of the issue you're experiencing. Our trained representatives will work diligently to investigate and resolve your concern promptly.\n",
    "\n",
    "    **Promotions:**\n",
    "    - First-time Customer Discount: As a special welcome offer for new customers joining the Axiomcart family, we are pleased to provide an exclusive 10% discount on your very first purchase. Simply use the promotional code 'FIRST10' during checkout to take advantage of this limited-time offer.\n",
    "\"\"\"\n",
    "\n",
    "# Test with multiple questions\n",
    "test_questions = [\n",
    "    \"Can I use MasterCard for payment?\",\n",
    "    \"How do I return an item?\",\n",
    "    \"Do you ship internationally?\",\n",
    "    \"Is there a discount for new customers?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    input_text = f\"\"\"                    \n",
    "                    SystemPrompt: \n",
    "                        You are a helpful and professional customer service AI assistant for Axiomcart, an e-commerce platform. \n",
    "                        Your role is to provide comprehensive, detailed, and thorough responses to customer inquiries based on the company's policies and procedures. \n",
    "                        You are very spontaneous and humorous, always maintaining a friendly and professional tone. \n",
    "                        You provide concise and accurate answers, ensuring that customers feel valued and understood.\n",
    "\n",
    "                        {KNOWLEDGE_BASE}\n",
    "\n",
    "                    UserQuery:\n",
    "                        {question}\n",
    "\n",
    "                    Response:\n",
    "                  \"\"\"\n",
    "    \n",
    "    # Tokenize and generate response\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs, \n",
    "            temperature=0.7,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the response part\n",
    "    response = response.split(\"Response:\")[-1].strip()\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cecfcda",
   "metadata": {},
   "source": [
    "Since LLM is a probablistic model, you might notice that the base model response are\n",
    "1. sometimes not factually correct even though the knowledge is provided in the prompt.\n",
    "2. sometimes incorrect reponse.\n",
    "3. Though the prompt asks the responses to be concise, due to length of the prompt, sometimes these information get lost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e225408",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Training Data\n",
    "\n",
    "Let's create a dataset of FAQ questions and answers specific to Axiomcart. This will be used to train our model to respond like a customer support assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "104fa388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Enhanced Dataset Statistics:\n",
      "- Training dataset: 13 examples\n",
      "- Evaluation dataset: 4 examples\n",
      "- Total: 17 examples\n",
      "- Coverage: Account Management, Payments, Orders, Returns, Support, Shipping, Promotions, Security, Products\n",
      "\n",
      "üîç Sample training data point:\n",
      "Instruction: What is your return policy?\n",
      "Response: Got buyer's remorse? It happens! üòÖ We offer a 30-day return policy for items in original condition w...\n",
      "\n",
      "üß™ Sample evaluation data point:\n",
      "Instruction: How can I track my order?\n",
      "Response: The eternal 'where's my stuff?' question! üì¶ Once processed, you'll get a tracking number via email a...\n"
     ]
    }
   ],
   "source": [
    "faq_data = [\n",
    "    # Account Management\n",
    "    {\"instruction\": \"How do I create an account on Axiomcart?\", \"response\": \"Creating an account is super easy! üéâ Navigate to our sign-up page, fill in your details (name, email, secure password), then verify your email. Click the verification link we send you and voil√† - welcome to the Axiomcart family! üöÄ\"},\n",
    "    \n",
    "    {\"instruction\": \"I forgot my password, how can I reset it?\", \"response\": \"Happens to the best of us! ü§¶‚Äç‚ôÄÔ∏è Just click 'Forgot Password' on our login page and we'll email you reset instructions. Follow the link to create a new secure password - maybe avoid 'password123' this time! üòâüîê\"},\n",
    "    \n",
    "    # Payment Methods\n",
    "    {\"instruction\": \"What payment methods does Axiomcart accept?\", \"response\": \"We're the Swiss Army knife of payments! üí≥ We accept all major credit cards (Visa, MasterCard, American Express), PayPal, and bank transfers. Basically, we've got more payment options than a food court has restaurants! üçïüí∞\"},\n",
    "    \n",
    "    {\"instruction\": \"Is it safe to save my credit card information?\", \"response\": \"Absolutely! Your payment info is locked up tighter than Fort Knox! üè∞ We use industry-leading encryption and PCI DSS compliance standards - like having a digital bodyguard for your financial info. We take security more seriously than a sommelier takes wine! üç∑üõ°Ô∏è\"},\n",
    "    \n",
    "    # Order Management\n",
    "    {\"instruction\": \"How can I track my order?\", \"response\": \"The eternal 'where's my stuff?' question! üì¶ Once processed, you'll get a tracking number via email automatically. Use it on our website or the carrier's portal to follow your package's journey - it's like GPS for goodies! üó∫Ô∏è‚ú®\"},\n",
    "    \n",
    "    {\"instruction\": \"How long does shipping usually take?\", \"response\": \"We're faster than your morning coffee delivery! ‚è∞ Domestic orders: 3-5 days standard, 1-2 days express. International shipping takes 7-14 days depending on location and customs - time for your package to collect passport stamps! üåç‚úàÔ∏è\"},\n",
    "    \n",
    "    {\"instruction\": \"Can I change or cancel my order after placing it?\", \"response\": \"Changed your mind? We totally get it! üé≠ You have 24 hours to modify or cancel, unless it's already processing. After that, contact our customer service team - we're basically order-modification wizards! ü™Ñ‚ö°\"},\n",
    "    \n",
    "    # Returns & Refunds\n",
    "    {\"instruction\": \"What is your return policy?\", \"response\": \"Got buyer's remorse? It happens! üòÖ We offer a 30-day return policy for items in original condition with tags. Contact customer service for return authorization and step-by-step instructions - we won't judge your shopping decisions! üõçÔ∏èüí≠\"},\n",
    "    \n",
    "    {\"instruction\": \"How do I return a defective item?\", \"response\": \"A defective item is totally unacceptable! üò§ Contact our customer service immediately with your order number and photos. We'll arrange free return shipping and send a replacement or refund - defective items get VIP treatment! üì¶‚ú®\"},\n",
    "    \n",
    "    # Customer Support\n",
    "    {\"instruction\": \"How can I contact customer support?\", \"response\": \"We're easier to reach than your favorite pizza place! üìûüçï Email us at support@axiomcart.com or use our live chat on the website. We're standing by like customer service superheroes, coffee in hand, ready to help! ‚òïü¶∏‚Äç‚ôÄÔ∏è\"},\n",
    "    \n",
    "    {\"instruction\": \"What are your customer service hours?\", \"response\": \"We're practically nocturnal! ü¶â Live chat and email support are 24/7 because questions don't follow schedules. Phone support: Monday-Friday 8 AM-8 PM EST, weekends 10 AM-6 PM EST. We're here more than your favorite coffee shop! ‚òï‚è∞\"},\n",
    "    \n",
    "    # Shipping & International\n",
    "    {\"instruction\": \"Does Axiomcart ship internationally?\", \"response\": \"Around the world in 50+ countries! üåç‚úàÔ∏è We offer comprehensive international shipping because awesome products deserve to see the world. Rates and timeframes vary by location - we haven't figured out teleportation yet! üöÄüì¶\"},\n",
    "    \n",
    "    {\"instruction\": \"Do I have to pay customs fees for international orders?\", \"response\": \"Ah, the customs question! üõÉ Sometimes your country charges duties and taxes - think of it as your package's entry fee. These fees are determined by your local customs authority and are the customer's responsibility - international shopping's adventure tax! üåçüí∞\"},\n",
    "    \n",
    "    # Promotions & Discounts\n",
    "    {\"instruction\": \"Are there any discounts for first-time customers?\", \"response\": \"Welcome to the party! üéâ New customers get an exclusive 10% discount on their first purchase. Just use code 'FIRST10' at checkout - it's like a secret handshake for savings! üí∞‚ú®\"},\n",
    "    \n",
    "    {\"instruction\": \"Do you have a loyalty program?\", \"response\": \"You bet! üåü Earn points with every purchase, redeem for discounts, get early sale access and birthday surprises. The more you shop, the more perks you unlock - like leveling up in a game with useful rewards! üéÆüéÅ\"},\n",
    "    \n",
    "    # Security & Privacy\n",
    "    {\"instruction\": \"How secure is my personal information on Axiomcart?\", \"response\": \"Your data is more secure than the Crown Jewels! üëëüîê We use industry-standard encryption and strict privacy policies. We don't share, sell, or distribute your data to third parties without consent - your secrets are safe with us! ü§ù‚ú®\"},\n",
    "    \n",
    "    # Product & Inventory\n",
    "    {\"instruction\": \"How do I know if an item is in stock?\", \"response\": \"Our inventory updates faster than small-town gossip! üì¢ Check product pages for real-time availability - 'Add to Cart' means we've got it. Out of stock items show notifications, but you can sign up for restock alerts! üì¶‚úÖ\"}\n",
    "]\n",
    "\n",
    "# Create balanced train/test split\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Combine and shuffle all data for better distribution\n",
    "all_data = faq_data.copy()\n",
    "random.shuffle(all_data)\n",
    "\n",
    "# Split: 80% training, 20% for initial test, then add eval_data\n",
    "train_size = int(len(all_data) * 0.8)\n",
    "train_data = all_data[:train_size]\n",
    "test_data = all_data[train_size:]\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "print(f\"üìö Enhanced Dataset Statistics:\")\n",
    "print(f\"- Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"- Evaluation dataset: {len(eval_dataset)} examples\")\n",
    "print(f\"- Total: {len(train_dataset) + len(eval_dataset)} examples\")\n",
    "print(f\"- Coverage: Account Management, Payments, Orders, Returns, Support, Shipping, Promotions, Security, Products\")\n",
    "\n",
    "print(f\"\\nüîç Sample training data point:\")\n",
    "print(f\"Instruction: {train_dataset[0]['instruction']}\")\n",
    "print(f\"Response: {train_dataset[0]['response'][:100]}...\")\n",
    "\n",
    "print(f\"\\nüß™ Sample evaluation data point:\")\n",
    "print(f\"Instruction: {eval_dataset[0]['instruction']}\")\n",
    "print(f\"Response: {eval_dataset[0]['response'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9515e1aa",
   "metadata": {},
   "source": [
    "## Step 5: Set Up Fine-Tuning Configuration\n",
    "\n",
    "Now we'll configure LoRA (Low-Rank Adaptation) for efficient fine-tuning. LoRA allows us to fine-tune large models efficiently by only updating a small number of parameters.\n",
    "\n",
    "**What is LoRA?**  \n",
    "LoRA is a parameter-efficient fine-tuning method. Instead of updating all the weights of a large model, it injects small trainable matrices (of low rank) into certain layers, drastically reducing the number of trainable parameters. This makes fine-tuning faster and less resource-intensive.\n",
    "\n",
    "**Configuration:**  \n",
    "| Parameter         | Description                                                                                      | Example Value              |\n",
    "|-------------------|--------------------------------------------------------------------------------------------------|----------------------------|\n",
    "| **r**             | Controls the size of the low-rank matrices. Higher values mean more capacity but more parameters. | `16`                       |\n",
    "| **lora_alpha**    | Scales the LoRA weights, affecting how much they influence the model.                             | `32`                       |\n",
    "| **lora_dropout**  | Adds regularization to prevent overfitting.                                                      | `0.1`                      |\n",
    "| **bias**          | Whether to train bias terms in the layers (here, not training them).                             | `\"none\"`                   |\n",
    "| **task_type**     | Specifies the task (here, causal language modeling, i.e., predicting the next word).             | `\"CAUSAL_LM\"`              |\n",
    "| **modules_to_save** | Ensures some modules (like the output head and embedding layer) are also trainable.             | `[\"lm_head\", \"embed_token\"]` |\n",
    "\n",
    "`peft` stands for `Parameter-Efficient Fine-Tuning`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81d8caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA configuration:\n",
      "- Rank (r): 16\n",
      "- Alpha: 32\n",
      "- Dropout: 0.1\n",
      "trainable params: 137,216,000 || all params: 631,248,768 || trainable%: 21.7372\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                    # Rank of adaptation\n",
    "    lora_alpha=32,           # LoRA scaling parameter\n",
    "    lora_dropout=0.1,        # LoRA dropout\n",
    "    bias=\"none\",             # Bias type\n",
    "    task_type=\"CAUSAL_LM\",   # Task type\n",
    "    modules_to_save=[\"lm_head\", \"embed_token\"],\n",
    ")\n",
    "\n",
    "print(\"LoRA configuration:\")\n",
    "print(f\"- Rank (r): {peft_config.r}\")\n",
    "print(f\"- Alpha: {peft_config.lora_alpha}\")\n",
    "print(f\"- Dropout: {peft_config.lora_dropout}\")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a4eab",
   "metadata": {},
   "source": [
    "## Step 6: Configure Training Arguments\n",
    "\n",
    "Let's set up the training parameters. These control how the model will be trained.\n",
    "These training arguments control the training loop, checkpointing, and evaluation for your fine-tuning process:\n",
    "\n",
    "| Argument                       | Description                                                                                                         |\n",
    "|---------------------------------|---------------------------------------------------------------------------------------------------------------------|\n",
    "| **output_dir**                  | Directory where model checkpoints and results will be saved.                                                         |\n",
    "| **per_device_train_batch_size** | Number of samples per batch, per device (CPU/GPU).                                                                  |\n",
    "| **gradient_accumulation_steps** | Number of steps to accumulate gradients before updating model weights. This helps simulate larger batch sizes.       |\n",
    "| **num_train_epochs**            | Number of times the model will iterate over the entire training dataset.                                             |\n",
    "| **learning_rate**               | The step size used for updating model weights during training.                                                       |\n",
    "| **fp16**                        | Whether to use 16-bit floating point precision for faster training (set to `False` for CPU).                        |\n",
    "| **save_steps**                  | How often (in steps) to save a model checkpoint.                                                                    |\n",
    "| **logging_steps**               | How often (in steps) to log training metrics.                                                                       |\n",
    "| **remove_unused_columns**       | If `False`, keeps all columns in the dataset (useful for custom data formats).                                      |\n",
    "| **eval_strategy**               | When to run evaluation (e.g., at the end of every epoch).                                                           |\n",
    "\n",
    "These settings are passed to the Trainer (or SFTTrainer) to manage the training process, saving, and evaluation of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275d17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "- Epochs: 40\n",
      "- Batch size: 4\n",
      "- Learning rate: 0.0001\n",
      "- Gradient accumulation steps: 4\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",              # Directory to save results\n",
    "    per_device_train_batch_size=4,       # Batch size per device\n",
    "    gradient_accumulation_steps=4,       # Steps to accumulate gradients\n",
    "    num_train_epochs=40,                 # Number of training epochs\n",
    "    learning_rate=0.0001,                # Learning rate\n",
    "    fp16=False,                          # Disable FP16 for CPU\n",
    "    save_steps=2,                        # Save model every N steps\n",
    "    logging_steps=3,                     # Log every N steps\n",
    "    remove_unused_columns=False,         # Keep all columns\n",
    "    eval_strategy=\"epoch\",               # Evaluation strategy\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"- Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"- Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"- Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"- Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713505b5",
   "metadata": {},
   "source": [
    "## Step 7: Initialize the Trainer\n",
    "\n",
    "We'll use the SFTTrainer (Supervised Fine-Tuning Trainer) from the TRL library, which is specifically designed for fine-tuning language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a9a3941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00<00:00, 3679.71 examples/s]\n",
      "Adding EOS to train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00<00:00, 3925.55 examples/s]\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00<00:00, 1192.97 examples/s]\n",
      "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00<00:00, 4761.26 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1476.09 examples/s]\n",
      "Adding EOS to eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1464.24 examples/s]\n",
      "Tokenizing eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 748.95 examples/s]\n",
      "Truncating eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1265.82 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully!\n",
      "Training dataset size: 13\n"
     ]
    }
   ],
   "source": [
    "# Define formatting function for SFTTrainer\n",
    "def formatting_func(data):\n",
    "    return f\"Instruction: {data['instruction']}\\nResponse: {data['response']}\"\n",
    "\n",
    "# SFTTrainer for supervised fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    formatting_func=formatting_func,  # Function to format the data\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# Set the tokenizer for the trainer\n",
    "trainer.tokenizer = tokenizer\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b5ef3",
   "metadata": {},
   "source": [
    "## Step 8: Start Training\n",
    "\n",
    "Now let's start the fine-tuning process. This will take a few minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beb2d739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n",
      "This may take a few minutes depending on your hardware...\n",
      "You could reduce the number of epochs if you want to speed up the process. However, note that this may affect the quality of the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 05:18, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.217403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.126611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.951100</td>\n",
       "      <td>3.056066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.951100</td>\n",
       "      <td>3.003057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.951100</td>\n",
       "      <td>2.965574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.195800</td>\n",
       "      <td>2.942489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.195800</td>\n",
       "      <td>2.932928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.195800</td>\n",
       "      <td>2.933825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.689400</td>\n",
       "      <td>2.941523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.689400</td>\n",
       "      <td>2.953161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.689400</td>\n",
       "      <td>2.967345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.339700</td>\n",
       "      <td>2.983440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.339700</td>\n",
       "      <td>3.000632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.339700</td>\n",
       "      <td>3.018354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.075200</td>\n",
       "      <td>3.036310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.075200</td>\n",
       "      <td>3.054298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.075200</td>\n",
       "      <td>3.072240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.869600</td>\n",
       "      <td>3.090051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.869600</td>\n",
       "      <td>3.107156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.869600</td>\n",
       "      <td>3.123521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.708300</td>\n",
       "      <td>3.138750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.708300</td>\n",
       "      <td>3.153051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.708300</td>\n",
       "      <td>3.166341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.584500</td>\n",
       "      <td>3.178739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.584500</td>\n",
       "      <td>3.190485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.584500</td>\n",
       "      <td>3.201747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.488600</td>\n",
       "      <td>3.212517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.488600</td>\n",
       "      <td>3.222780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.488600</td>\n",
       "      <td>3.232596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>3.241936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>3.250544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>3.258457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>3.265574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>3.271851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>3.277302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>3.281828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>3.285485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>3.288232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.312600</td>\n",
       "      <td>3.290049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.312600</td>\n",
       "      <td>3.290947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(\"This may take a few minutes depending on your hardware...\")\n",
    "print(\"You could reduce the number of epochs if you want to speed up the process. However, note that this may affect the quality of the model.\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8013305a",
   "metadata": {},
   "source": [
    "## Step 9: Save the Fine-Tuned Model\n",
    "\n",
    "Let's save our fine-tuned model so we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2540985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ./fine-tuned-qwen-0.5b\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model_save_path = \"./fine-tuned-qwen-0.5b\"\n",
    "trainer.save_model(model_save_path)\n",
    "\n",
    "print(f\"Model saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe5a6a",
   "metadata": {},
   "source": [
    "## Step 10: Test the Fine-Tuned Model\n",
    "\n",
    "Now let's test our fine-tuned model and compare it with the base model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f29373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the accepted payment methods on Axiomcart?\n",
      "============================================================\n",
      "We are a Swiss Army knife of payments! The Swiss Army knife of payments! Axiomcart accept all major credit cards (Visa, MasterCard, American Express), PayPal, and bank transfers. We're in the Swiss Army knife of payments! The Swiss Army knife of payments! Axiomcart accept all major credit cards (Visa, MasterCard, American Express), PayPal, and bank transfers. We're in the Swiss Army knife of payments! The Swiss Army knife of payments! Axiomcart accept all major credit cards (Visa, MasterCard, American Express), PayPal, and bank transfers. We're in the Swiss Army knife of payments! The Swiss Army knife of payments! Axiomcart accept all major credit cards (Visa, MasterCard, American Express), PayPal, and bank transfers. We're in the Swiss Army knife of payments! The Swiss Army knife of payments! Axiomcart accept all major credit cards (Visa, MasterCard, American Express), PayPal, and bank transfers. We're in the Swiss Army knife of payments! The Swiss Army knife of payments! Axiomcart accept all major credit cards (Visa, MasterCard, American Express), PayPal, and bank transfers. We're in the Swiss Army knife of payments! The Swiss Army knife of payments! Axiomcart accept all major credit cards (Visa, MasterCard, American Express), PayPal, and bank transfers. We're in the Swiss Army knife\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(model_save_path).to(device)\n",
    "\n",
    "# Test with the same question we used for the base model\n",
    "test_question = \"What are the accepted payment methods on Axiomcart?\"\n",
    "input_text = f\"Instruction: {test_question}\\nResponse:\"\n",
    "\n",
    "print(f\"Q: {test_question}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate response with fine-tuned model\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = fine_tuned_model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=300,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "fine_tuned_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract the response part\n",
    "fine_tuned_response = fine_tuned_response.split(\"Response:\")[-1].strip()\n",
    "\n",
    "print(fine_tuned_response)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490480ef",
   "metadata": {},
   "source": [
    "## Step 11: Test with Additional Questions\n",
    "\n",
    "Let's test our fine-tuned model with a few more questions to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "768b7caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTING MULTIPLE QUESTIONS\n",
      "============================================================\n",
      "\n",
      "1. Question: How do I return an item?\n",
      "   Answer: You can return items with your order number and contact information. Follow the return policy and instructions for your shopping cart. We're the party with a handshake - get in touch with us! üì¢‚úÖ\n",
      "------------------------------------------------------------\n",
      "\n",
      "2. Question: Do you ship internationally?\n",
      "   Answer: We're faster than your local shopping website! üõçÔ∏è‚ú® We're shipping to more than 100 countries! üåç‚úàÔ∏è\n",
      "------------------------------------------------------------\n",
      "\n",
      "3. Question: Is there a discount for new customers?\n",
      "   Answer: Welcome to our loyalty program! New customers get an exclusive 10% discount on their first purchase. Just use code 'FIRST10' at checkout - it's like a secret handshake!\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple questions\n",
    "test_questions = [\n",
    "    \"How do I return an item?\",\n",
    "    \"Do you ship internationally?\",\n",
    "    \"Is there a discount for new customers?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ TESTING MULTIPLE QUESTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    input_text = f\"Instruction: {question}\\nResponse:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=300, \n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n{i}. Question: {question}\")\n",
    "    print(f\"   Answer: {response.split('Response:')[1].strip() if 'Response:' in response else response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1196674e",
   "metadata": {},
   "source": [
    "## üéâ Workshop Summary\n",
    "\n",
    "Congratulations! You have successfully:\n",
    "\n",
    "1. ‚úÖ Loaded a pre-trained language model (Qwen2-0.5B)\n",
    "2. ‚úÖ Prepared custom training data for Axiomcart\n",
    "3. ‚úÖ Tested the base model performance\n",
    "4. ‚úÖ Configured LoRA for efficient fine-tuning\n",
    "5. ‚úÖ Fine-tuned the model on domain-specific data\n",
    "6. ‚úÖ Compared base vs fine-tuned model performance\n",
    "7. ‚úÖ Saved the fine-tuned model for future use\n",
    "\n",
    "### Key Takeaways:\n",
    "- **LoRA** enables efficient fine-tuning by updating only a small fraction of parameters\n",
    "- **Domain-specific training** significantly improves model performance for specific use cases\n",
    "- **Proper data formatting** is crucial for successful fine-tuning\n",
    "- **Comparison testing** helps validate the effectiveness of fine-tuning\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different LoRA configurations (r, alpha, dropout)\n",
    "- Try larger datasets or different domains\n",
    "- Explore evaluation metrics for more systematic comparison\n",
    "- Deploy the model as a customer service chatbot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
