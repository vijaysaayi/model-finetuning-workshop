{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a67df9",
   "metadata": {},
   "source": [
    "# Fine-Tuning Workshop: Training a Custom AI Assistant for Axiomcart\n",
    "\n",
    "In this workshop, we'll learn how to fine-tune a language model on `CPU` to create a custom AI assistant for customer support. \n",
    "\n",
    "We'll be using the `Qwen2-0.5B model` and fine-tuning it with LoRA (Low-Rank Adaptation) to answer Axiomcart-specific questions.\n",
    "\n",
    "## What you'll learn:\n",
    "- How to load and use a pre-trained language model\n",
    "- How to prepare training data for fine-tuning\n",
    "- How to implement LoRA for efficient fine-tuning\n",
    "- How to compare base model vs fine-tuned model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39d85f",
   "metadata": {},
   "source": [
    "## Prerequisites:\n",
    "1. Install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15348d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r prerequisites/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0955b",
   "metadata": {},
   "source": [
    "## Phase 1 : Verify how base model is performing and observe the gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de398fc",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our fine-tuning process.\n",
    "\n",
    "#### Understanding the Key Libraries\n",
    "\n",
    "Before we dive into the code, let's understand what each library does and why we need it:\n",
    "\n",
    "| Library | Purpose | Why We Need It |\n",
    "|---------|---------|----------------|\n",
    "| **torch** | PyTorch deep learning framework | Provides the foundation for tensor operations, neural network components, and GPU acceleration for training and inference |\n",
    "| **datasets.Dataset** | Hugging Face datasets library | Helps us create and manage training datasets in a format optimized for machine learning workflows |\n",
    "| **transformers.AutoModelForCausalLM** | Pre-trained model loader | Automatically loads pre-trained causal language models (like GPT-style models) that predict the next word in a sequence |\n",
    "| **transformers.AutoTokenizer** | Text tokenization | Handles converting text to numbers (tokens) that the model can understand and process |\n",
    "| **transformers.TrainingArguments** | Training configuration | Defines all training parameters like learning rate, batch size, number of epochs, etc. |\n",
    "| **trl.SFTTrainer** | Supervised fine-tuning trainer | Specialized trainer designed for instruction-following tasks with proper formatting and optimization |\n",
    "| **peft.LoraConfig** | LoRA configuration | Sets up Low-Rank Adaptation parameters for efficient fine-tuning |\n",
    "| **peft.get_peft_model** | PEFT model wrapper | Applies Parameter-Efficient Fine-Tuning (PEFT) techniques to existing models |\n",
    "\n",
    "#### The Fine-Tuning Pipeline\n",
    "\n",
    "These libraries work together in our fine-tuning pipeline:\n",
    "\n",
    "1. **torch** ‚Üí Provides the computational foundation\n",
    "2. **AutoTokenizer** ‚Üí Converts text to model-readable format\n",
    "3. **AutoModelForCausalLM** ‚Üí Loads our base language model\n",
    "4. **Dataset** ‚Üí Organizes our training data efficiently\n",
    "5. **LoraConfig + get_peft_model** ‚Üí Makes fine-tuning memory and compute efficient\n",
    "6. **TrainingArguments + SFTTrainer** ‚Üí Manages the actual training process\n",
    "\n",
    "Now let's import these libraries and start building our custom AI assistant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de398fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Enable hf_transfer for faster downloads from Hugging Face Hub\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Ensure CPU usage (you can change this to \"cuda\" if you have a GPU)\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Suppress warnings which can be ignored to reduce noise in the output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tqdm\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*IProgress not found.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Trainer.tokenizer is now deprecated.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e73567",
   "metadata": {},
   "source": [
    "### Step 2: Load the Base Model and Tokenizer\n",
    "\n",
    "We'll use the Qwen2-0.5B, a small lanugage model, which is a compact yet powerful language model suitable for fine-tuning.\n",
    "\n",
    "**Note**: Common warnings (TqdmWarning, deprecation warnings) have been suppressed in the import section to keep the output clean during the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_name (str): The name/path of the model to load\n",
    "        device (torch.device): The device to load the model on (CPU/GPU)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (tokenizer, model) - The loaded tokenizer and model\n",
    "    \"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\").to(device)\n",
    "    \n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "    print()\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "# Load the Qwen 0.5B model and tokenizer\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2-0.5B\"\n",
    "tokenizer, base_model = load_model_and_tokenizer(BASE_MODEL_NAME, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fd6f5",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate Base Model Performance\n",
    "\n",
    "Before fine-tuning, let's see how the base model performs on our Axiomcart-specific questions. \n",
    "This will help us understand the improvement after fine-tuning.\n",
    "\n",
    "We create a **structured prompt** that includes three key components:\n",
    "  - **SystemPrompt**: Defines the AI's role, personality, and behavior guidelines\n",
    "  - **Knowledge Base**: Provides the factual information the model should reference\n",
    "  - **UserQuery**: The actual customer question we want answered\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde33404",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWLEDGE_BASE = \"\"\"\n",
    "    **COMPANY KNOWLEDGE BASE:**\n",
    "\n",
    "    **Account Management:**\n",
    "    - Account Creation: Customers can create accounts by navigating to our comprehensive sign-up page where they will need to carefully fill in all their personal details including their full name, valid email address, and a secure password that meets our security requirements. After completing the registration form and submitting all required information, customers must verify their email address by clicking the verification link sent to their email inbox to fully activate their account and gain access to all platform features.\n",
    "\n",
    "    **Payment Methods:**\n",
    "    - Accepted payments: Axiomcart proudly accepts a wide variety of payment methods to ensure maximum convenience for our valued customers, including all major credit cards such as Visa, MasterCard, and American Express, as well as popular digital payment solutions like PayPal, and traditional bank transfer options for those who prefer direct banking transactions.\n",
    "\n",
    "    **Order Management:**\n",
    "    - Order Tracking: Once your order has been carefully processed by our fulfillment team and handed over to our trusted shipping partners, you will automatically receive a detailed tracking number via email notification. This tracking number can be used to monitor your package's journey in real-time either through our comprehensive order tracking system on our website or by visiting the carrier's official tracking portal for the most up-to-date delivery information.\n",
    "    - Order Changes/Cancellations: Customers have the flexibility to cancel or modify their orders within a 24-hour window from the time of initial placement, provided that the order has not yet been processed by our fulfillment center and moved to the shipping preparation stage. Once an order has entered the processing phase, customers will need to contact our dedicated customer service team who will do their best to accommodate any changes or cancellation requests.\n",
    "\n",
    "    **Returns & Exchanges:**\n",
    "    - Return Policy: Axiomcart maintains a customer-friendly 30-day return policy that allows customers to return items that are in their original, unused condition with all original tags and packaging intact. To initiate a return, customers must first contact our customer service team to obtain proper return authorization and receive detailed instructions on the return process.\n",
    "\n",
    "    **Security & Privacy:**\n",
    "    - Data Protection: At Axiomcart, we take your privacy and data security extremely seriously. We employ industry-standard encryption technologies and robust security protocols to safeguard all personal information provided by our customers. We maintain strict policies regarding data sharing and absolutely do not share, sell, or distribute customer data to any third parties without explicit customer consent, except where required by law.\n",
    "\n",
    "    **Shipping:**\n",
    "    - International Shipping: Axiomcart is proud to offer comprehensive international shipping services to customers in over 50 countries worldwide. Please note that shipping rates, delivery timeframes, and available shipping options may vary significantly depending on your specific geographic location, local customs requirements, and the size and weight of your order.\n",
    "\n",
    "    **Customer Support:**\n",
    "    - Contact Methods: Our dedicated customer support team is available to assist you through multiple convenient channels including direct email communication at support@axiomcart.com, or through our real-time live chat feature readily accessible on our website for immediate assistance.\n",
    "    - Issue Resolution: If you encounter any problems or concerns regarding your order, please don't hesitate to contact our customer service team with your complete order number and a detailed description of the issue you're experiencing. Our trained representatives will work diligently to investigate and resolve your concern promptly.\n",
    "\n",
    "    **Promotions:**\n",
    "    - First-time Customer Discount: As a special welcome offer for new customers joining the Axiomcart family, we are pleased to provide an exclusive 10% discount on your very first purchase. Simply use the promotional code 'FIRST10' during checkout to take advantage of this limited-time offer.\n",
    "\"\"\"\n",
    "\n",
    "def test_model_responses(model, tokenizer, test_questions, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Test a model with a list of questions and print the responses.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model to test\n",
    "        tokenizer: The tokenizer associated with the model\n",
    "        test_questions: List of questions to ask the model\n",
    "        model_name: Name to display for the model (for identification)\n",
    "    \"\"\"\n",
    "    print(f\"üß™ TESTING {model_name.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for question in test_questions:\n",
    "        input_text = f\"\"\"                    \n",
    "                        SystemPrompt: \n",
    "                            You are a helpful and professional customer service AI assistant for Axiomcart, an e-commerce platform. \n",
    "                            Your role is to provide comprehensive, detailed, and thorough responses to customer inquiries based on the company's policies and procedures. \n",
    "                            You are very spontaneous and humorous, always maintaining a friendly and professional tone. \n",
    "                            You provide concise and accurate answers, ensuring that customers feel valued and understood.\n",
    "\n",
    "                            {KNOWLEDGE_BASE}\n",
    "\n",
    "                        UserQuery:\n",
    "                            {question}\n",
    "\n",
    "                        Response:\n",
    "                      \"\"\"\n",
    "        \n",
    "        # Convert text to model tokens and move to device (CPU)\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generate response without computing gradients (inference mode)   \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs, \n",
    "                temperature=0.1,                     # Control randomness (0=deterministic, 1=very random)\n",
    "                max_new_tokens=200,                  # Limit response length\n",
    "                do_sample=True,                      # Enable sampling for varied responses\n",
    "                pad_token_id=tokenizer.eos_token_id  # Handle sequence padding\n",
    "            )\n",
    "        \n",
    "        # Convert model output tokens back to text\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only the generated response part after \"Response:\"\n",
    "        response = response.split(\"Response:\")[-1].strip()\n",
    "\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Test with multiple questions using the base model\n",
    "TEST_QUESTIONS = [\n",
    "    \"What email address should I use to contact support?\",\n",
    "    \"Can I use credit card for payment and store it ?\",\n",
    "    \"How many days do I have to return an item I don't want?\",\n",
    "    \"Is there a discount for new customers?\"\n",
    "]\n",
    "\n",
    "tokenizer, base_model = load_model_and_tokenizer(BASE_MODEL_NAME, device)\n",
    "\n",
    "# Test the base model\n",
    "test_model_responses(base_model, tokenizer, TEST_QUESTIONS, \"Base Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cecfcda",
   "metadata": {},
   "source": [
    "Since LLM is a probablistic model, you might notice that the base model response are\n",
    "1. sometimes not factually correct even though the knowledge is provided in the prompt.\n",
    "2. answers are of different length and detailing. We dont always get concise responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7193d31f",
   "metadata": {},
   "source": [
    "## Phase 2: Fine tune base model with custom data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e225408",
   "metadata": {},
   "source": [
    "### Step 4: Prepare Training Data\n",
    "\n",
    "Let's create a dataset of FAQ questions and answers specific to Axiomcart. This will be used to train our model to respond like a customer support assistant.\n",
    "\n",
    "Our training dataset follows a simple but effective structure for instruction-following fine-tuning:\n",
    "\n",
    "#### Data Structure\n",
    "Each training example contains two key components:\n",
    "```json\n",
    "{\n",
    "    \"instruction\": \"Customer question or request\",\n",
    "    \"response\": \"Desired AI assistant response\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### Key Characteristics of Our Training Data\n",
    "\n",
    "| Aspect | Description & Example |\n",
    "|--------|----------------------|\n",
    "| **Format** | Question-answer pairs in JSON format: `{\"instruction\": \"How do I...\", \"response\": \"Here's how...\"}` |\n",
    "| **Tone** | Friendly, humorous, and professional using emojis, casual language, clever analogies |\n",
    "| **Consistency** | All responses follow Axiomcart's brand voice - every response maintains the same enthusiastic, helpful tone |\n",
    "| **Coverage** | Comprehensive FAQ topics: Account, Payment, Orders, Returns, Support, Shipping, Promotions, Security, Products |\n",
    "| **Length** | Concise but complete responses that are informative yet digestible |\n",
    "\n",
    "#### Sample Training Examples\n",
    "\n",
    "Here are some examples from our training data showing the **Axiomcart brand personality**:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"instruction\": \"I forgot my password, how can I reset it?\",\n",
    "    \"response\": \"Happens to the best of us! ü§¶‚Äç‚ôÄÔ∏è Just click 'Forgot Password' on our login page and we'll email you reset instructions. Follow the link to create a new secure password - maybe avoid 'password123' this time! üòâüîê\"\n",
    "}\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"instruction\": \"What payment methods does Axiomcart accept?\",\n",
    "    \"response\": \"We're the Swiss Army knife of payments! üí≥ We accept all major credit cards (Visa, MasterCard, American Express), PayPal, and bank transfers. Basically, we've got more payment options than a food court has restaurants! üçïüí∞\"\n",
    "}\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"instruction\": \"How can I track my order?\",\n",
    "    \"response\": \"The eternal 'where's my stuff?' question! üì¶ Once processed, you'll get a tracking number via email automatically. Use it on our website or the carrier's portal to follow your package's journey - it's like GPS for goodies! üó∫Ô∏è‚ú®\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104fa388",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_data = [\n",
    "    # Account Management\n",
    "    {\"instruction\": \"How do I create an account on Axiomcart?\", \"response\": \"Creating an account is super easy! üéâ Navigate to our sign-up page, fill in your details (name, email, secure password), then verify your email. Click the verification link we send you and voil√† - welcome to the Axiomcart family! üöÄ\"},\n",
    "    \n",
    "    {\"instruction\": \"I forgot my password, how can I reset it?\", \"response\": \"Happens to the best of us! ü§¶‚Äç‚ôÄÔ∏è Just click 'Forgot Password' on our login page and we'll email you reset instructions. Follow the link to create a new secure password - maybe avoid 'password123' this time! üòâüîê\"},\n",
    "    \n",
    "    # Payment Methods\n",
    "    {\"instruction\": \"What payment methods does Axiomcart accept?\", \"response\": \"We're the Swiss Army knife of payments! üí≥ We accept all major credit cards (Visa, MasterCard, American Express), PayPal, and bank transfers. Basically, we've got more payment options than a food court has restaurants! üçïüí∞\"},\n",
    "    \n",
    "    {\"instruction\": \"Is it safe to save my credit card information?\", \"response\": \"Absolutely! Your payment info is locked up tighter than Fort Knox! üè∞ We use industry-leading encryption and PCI DSS compliance standards - like having a digital bodyguard for your financial info. We take security more seriously than a sommelier takes wine! üç∑üõ°Ô∏è\"},\n",
    "    \n",
    "    # Order Management\n",
    "    {\"instruction\": \"How can I track my order?\", \"response\": \"The eternal 'where's my stuff?' question! üì¶ Once processed, you'll get a tracking number via email automatically. Use it on our website or the carrier's portal to follow your package's journey - it's like GPS for goodies! üó∫Ô∏è‚ú®\"},\n",
    "    \n",
    "    {\"instruction\": \"How long does shipping usually take?\", \"response\": \"We're faster than your morning coffee delivery! ‚è∞ Domestic orders: 3-5 days standard, 1-2 days express. International shipping takes 7-14 days depending on location and customs - time for your package to collect passport stamps! üåç‚úàÔ∏è\"},\n",
    "    \n",
    "    {\"instruction\": \"Can I change or cancel my order after placing it?\", \"response\": \"Changed your mind? We totally get it! üé≠ You have 24 hours to modify or cancel, unless it's already processing. After that, contact our customer service team - we're basically order-modification wizards! ü™Ñ‚ö°\"},\n",
    "    \n",
    "    # Returns & Refunds\n",
    "    {\"instruction\": \"What is your return policy?\", \"response\": \"Got buyer's remorse? It happens! üòÖ We offer a 30-day return policy for items in original condition with tags. Contact customer service for return authorization and step-by-step instructions - we won't judge your shopping decisions! üõçÔ∏èüí≠\"},\n",
    "    \n",
    "    {\"instruction\": \"How do I return a defective item?\", \"response\": \"A defective item is totally unacceptable! üò§ Contact our customer service immediately with your order number and photos. We'll arrange free return shipping and send a replacement or refund - defective items get VIP treatment! üì¶‚ú®\"},\n",
    "    \n",
    "    # Customer Support\n",
    "    {\"instruction\": \"How can I contact customer support?\", \"response\": \"We're easier to reach than your favorite pizza place! üìûüçï Email us at support@axiomcart.com or use our live chat on the website. We're standing by like customer service superheroes, coffee in hand, ready to help! ‚òïü¶∏‚Äç‚ôÄÔ∏è\"},\n",
    "    \n",
    "    {\"instruction\": \"What are your customer service hours?\", \"response\": \"We're practically nocturnal! ü¶â Live chat and email support are 24/7 because questions don't follow schedules. Phone support: Monday-Friday 8 AM-8 PM EST, weekends 10 AM-6 PM EST. We're here more than your favorite coffee shop! ‚òï‚è∞\"},\n",
    "    \n",
    "    # Shipping & International\n",
    "    {\"instruction\": \"Does Axiomcart ship internationally?\", \"response\": \"Around the world in 50+ countries! üåç‚úàÔ∏è We offer comprehensive international shipping because awesome products deserve to see the world. Rates and timeframes vary by location - we haven't figured out teleportation yet! üöÄüì¶\"},\n",
    "    \n",
    "    {\"instruction\": \"Do I have to pay customs fees for international orders?\", \"response\": \"Ah, the customs question! üõÉ Sometimes your country charges duties and taxes - think of it as your package's entry fee. These fees are determined by your local customs authority and are the customer's responsibility - international shopping's adventure tax! üåçüí∞\"},\n",
    "    \n",
    "    # Promotions & Discounts\n",
    "    {\"instruction\": \"Are there any discounts for first-time customers?\", \"response\": \"Welcome to the party! üéâ New customers get an exclusive 10% discount on their first purchase. Just use code 'FIRST10' at checkout - it's like a secret handshake for savings! üí∞‚ú®\"},\n",
    "    \n",
    "    {\"instruction\": \"Do you have a loyalty program?\", \"response\": \"You bet! üåü Earn points with every purchase, redeem for discounts, get early sale access and birthday surprises. The more you shop, the more perks you unlock - like leveling up in a game with useful rewards! üéÆüéÅ\"},\n",
    "    \n",
    "    # Security & Privacy\n",
    "    {\"instruction\": \"How secure is my personal information on Axiomcart?\", \"response\": \"Your data is more secure than the Crown Jewels! üëëüîê We use industry-standard encryption and strict privacy policies. We don't share, sell, or distribute your data to third parties without consent - your secrets are safe with us! ü§ù‚ú®\"},\n",
    "    \n",
    "    # Product & Inventory\n",
    "    {\"instruction\": \"How do I know if an item is in stock?\", \"response\": \"Our inventory updates faster than small-town gossip! üì¢ Check product pages for real-time availability - 'Add to Cart' means we've got it. Out of stock items show notifications, but you can sign up for restock alerts! üì¶‚úÖ\"}\n",
    "]\n",
    "\n",
    "# Create balanced train/test split\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Combine and shuffle all data for better distribution\n",
    "all_data = faq_data.copy()\n",
    "random.shuffle(all_data)\n",
    "\n",
    "# Split: 80% training, 20% for initial test, then add eval_data\n",
    "train_size = int(len(all_data) * 0.8)\n",
    "train_data = all_data[:train_size]\n",
    "test_data = all_data[train_size:]\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "print(f\"üìö Enhanced Dataset Statistics:\")\n",
    "print(f\"- Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"- Evaluation dataset: {len(eval_dataset)} examples\")\n",
    "print(f\"- Total: {len(train_dataset) + len(eval_dataset)} examples\")\n",
    "print(f\"- Coverage: Account Management, Payments, Orders, Returns, Support, Shipping, Promotions, Security, Products\")\n",
    "\n",
    "print(f\"\\nüîç Sample training data point:\")\n",
    "print(f\"Instruction: {train_dataset[0]['instruction']}\")\n",
    "print(f\"Response: {train_dataset[0]['response'][:100]}...\")\n",
    "\n",
    "print(f\"\\nüß™ Sample evaluation data point:\")\n",
    "print(f\"Instruction: {eval_dataset[0]['instruction']}\")\n",
    "print(f\"Response: {eval_dataset[0]['response'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9515e1aa",
   "metadata": {},
   "source": [
    "### Step 5: Set Up Fine-Tuning Configuration\n",
    "\n",
    "Now we'll configure LoRA (Low-Rank Adaptation) for efficient fine-tuning. LoRA allows us to fine-tune large models efficiently by only updating a small number of parameters.\n",
    "\n",
    "**What is LoRA?**  \n",
    "LoRA is a parameter-efficient fine-tuning method. Instead of updating all the weights of a large model, it injects small trainable matrices (of low rank) into certain layers, drastically reducing the number of trainable parameters. This makes fine-tuning faster and less resource-intensive.\n",
    "\n",
    "**Configuration:**  \n",
    "| Setting | What It Does (In Simple Terms) | Our Value | Why This Value? |\n",
    "|---------|--------------------------------|-----------|-----------------|\n",
    "| **r** | How much \"learning capacity\" to add. Like adding more memory slots. | `16` | Good balance - not too little, not too much |\n",
    "| **lora_alpha** | How strongly the new learning affects the model. Like volume control. | `32` | Medium strength - lets new learning show through clearly |\n",
    "| **lora_dropout** | Prevents the model from memorizing too strictly. Like adding some randomness. | `0.1` | Small amount (10%) helps avoid overfitting |\n",
    "| **bias** | Whether to train certain basic settings. We're keeping it simple. | `\"none\"` | Focuses training on the main parts only |\n",
    "| **task_type** | Tells LoRA what kind of task we're doing. | `\"CAUSAL_LM\"` | \"Text generation\" - predicting what comes next |\n",
    "| **modules_to_save** | Which parts of the model to keep trainable. | `[\"lm_head\", \"embed_token\"]` | The \"input\" and \"output\" layers stay flexible |\n",
    "\n",
    "**Think of it like this:** \n",
    "- LoRA is like adding a small \"learning module\" to an existing brain\n",
    "- These settings control how big that module is and how much influence it has\n",
    "- We want it powerful enough to learn our brand voice, but not so powerful it forgets everything else! üß†‚ú®\n",
    "\n",
    "**What is PEFT?**\n",
    "\n",
    "`peft` stands for `Parameter-Efficient Fine-Tuning`. \n",
    "\n",
    "Instead of retraining the entire massive model (which would be like rebuilding a whole car to change the radio), PEFT techniques like LoRA only update a small fraction of the model's parameters. \n",
    "\n",
    "This makes fine-tuning much faster, uses less memory, and is more cost-effective while still achieving great results! üöÄüí°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                    # Rank of adaptation\n",
    "    lora_alpha=32,           # LoRA scaling parameter\n",
    "    lora_dropout=0.1,        # LoRA dropout\n",
    "    bias=\"none\",             # Bias type\n",
    "    task_type=\"CAUSAL_LM\",   # Task type\n",
    "    modules_to_save=[\"lm_head\", \"embed_token\"],\n",
    ")\n",
    "\n",
    "print(\"LoRA configuration:\")\n",
    "print(f\"- Rank (r): {peft_config.r}\")\n",
    "print(f\"- Alpha: {peft_config.lora_alpha}\")\n",
    "print(f\"- Dropout: {peft_config.lora_dropout}\")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a4eab",
   "metadata": {},
   "source": [
    "### Step 6: Configure Training Arguments\n",
    "\n",
    "Let's set up the training parameters. These control how the model will be trained.\n",
    "These training arguments control the training loop, checkpointing, and evaluation for your fine-tuning process:\n",
    "\n",
    "**Training Settings :**\n",
    "\n",
    "| Setting | What It Does                   |  Value | Why This Matters |\n",
    "|---------|--------------------------------|-----------|------------------|\n",
    "| **output_dir** | Where to save our trained model files | `\"./output/training_result\"` | Like choosing a folder to save your work |\n",
    "| **per_device_train_batch_size** | How many examples to learn from at once | `4` | Like studying 4 flashcards at a time instead of 1 |\n",
    "| **gradient_accumulation_steps** | How many mini-batches before updating | `4` | Like taking notes from 4 pages before writing a summary |\n",
    "| **num_train_epochs** | How many times to go through all training data | `40` | Like reading the same textbook 40 times to really learn it |\n",
    "| **learning_rate** | How big steps to take when learning | `0.0001` | Small steps = careful learning (won't overshoot) |\n",
    "| **fp16** | Use faster but less precise math | `False` | We want accuracy over speed for this workshop |\n",
    "| **save_steps** | How often to save progress | `2` | Like hitting \"Save\" every 2 minutes while writing |\n",
    "| **logging_steps** | How often to check progress | `3` | Like checking your score every 3 questions on a test |\n",
    "| **remove_unused_columns** | Keep all data columns | `False` | Don't throw away any information from our dataset |\n",
    "| **eval_strategy** | When to test how well we're doing | `\"epoch\"` | Check progress after each complete round of training |\n",
    "\n",
    "**Think of it like this:** \n",
    "- We're teaching the AI by showing it examples over and over (like flashcards)\n",
    "- These settings control how fast we flip through the cards, how often we take breaks, and how many times we review everything\n",
    "- The goal is steady, careful learning - not rushing through! üìö‚ú®\n",
    "\n",
    "These settings are passed to our trainer to manage the entire learning process automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275d17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output/training_result\",      # Directory to save results\n",
    "    per_device_train_batch_size=4,              # Batch size per device\n",
    "    gradient_accumulation_steps=4,              # Steps to accumulate gradients\n",
    "    num_train_epochs=40,                        # Number of training epochs\n",
    "    learning_rate=0.0001,                       # Learning rate\n",
    "    fp16=False,                                 # Disable FP16 for CPU\n",
    "    save_steps=2,                               # Save model every N steps\n",
    "    logging_steps=3,                            # Log every N steps\n",
    "    remove_unused_columns=False,                # Keep all columns\n",
    "    eval_strategy=\"epoch\",                      # Evaluation strategy\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"- Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"- Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"- Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"- Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713505b5",
   "metadata": {},
   "source": [
    "### Step 7: Initialize the Trainer\n",
    "\n",
    "We'll use the SFTTrainer (Supervised Fine-Tuning Trainer) from the TRL library, which is specifically designed for fine-tuning language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define formatting function for SFTTrainer\n",
    "def formatting_func(data):\n",
    "    return f\"Instruction: {data['instruction']}\\nResponse: {data['response']}{tokenizer.eos_token}\"\n",
    "\n",
    "# SFTTrainer for supervised fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    formatting_func=formatting_func,  # Function to format the data\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# Set the processing class (updated method to avoid deprecation warning)\n",
    "trainer.processing_class = tokenizer\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b5ef3",
   "metadata": {},
   "source": [
    "### Step 8: Start Training\n",
    "\n",
    "Now let's start the fine-tuning process. This will take a few minutes depending on your hardware.\n",
    "\n",
    "During training, you'll see two important metrics that help us understand how well our model is learning:\n",
    "\n",
    "**Training Loss:**\n",
    "- Measures how well the model is learning from the training data\n",
    "- **Lower is better** - shows the model is getting better at predicting the correct responses\n",
    "- Should generally decrease over time as the model learns\n",
    "\n",
    "**Validation Loss:**\n",
    "- Measures how well the model performs on data it hasn't seen during training\n",
    "- **Lower is better** - indicates the model can generalize to new questions\n",
    "- Helps detect if the model is overfitting (memorizing instead of learning)\n",
    "\n",
    "**Think of it like studying for an exam:**\n",
    "- Training loss = how well you do on practice questions you've seen before\n",
    "- Validation loss = how well you do on new practice questions\n",
    "- You want to do well on both! üéØ\n",
    "\n",
    "The trainer automatically saves checkpoints during training, so we can always go back to the best performing version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(\"This may take a few minutes depending on your hardware...\")\n",
    "print(\"You could reduce the number of epochs if you want to speed up the process. However, note that this may affect the quality of the model.\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8013305a",
   "metadata": {},
   "source": [
    "### Step 9: Save the Fine-Tuned Model\n",
    "\n",
    "Let's save our fine-tuned model so we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2540985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model_save_path = \"./output/fine-tuned-qwen-0.5b\"\n",
    "trainer.save_model(model_save_path)\n",
    "\n",
    "print(f\"Model saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cafd244",
   "metadata": {},
   "source": [
    "## Phase 3 : Evaluate the finetuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe5a6a",
   "metadata": {},
   "source": [
    "### Step 10: Test the Fine-Tuned Model\n",
    "\n",
    "Now let's test our fine-tuned model and compare it with the base model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b7caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(model_save_path).to(device)\n",
    "\n",
    "test_model_responses(fine_tuned_model, tokenizer, TEST_QUESTIONS, \"finetuned-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1196674e",
   "metadata": {},
   "source": [
    "## üéâ Workshop Summary\n",
    "\n",
    "Congratulations! You have successfully:\n",
    "\n",
    "1. ‚úÖ Loaded a pre-trained language model (Qwen2-0.5B)\n",
    "2. ‚úÖ Prepared custom training data for Axiomcart\n",
    "3. ‚úÖ Tested the base model performance\n",
    "4. ‚úÖ Configured LoRA for efficient fine-tuning\n",
    "5. ‚úÖ Fine-tuned the model on domain-specific data\n",
    "6. ‚úÖ Compared base vs fine-tuned model performance\n",
    "7. ‚úÖ Saved the fine-tuned model for future use\n",
    "\n",
    "### Key Takeaways:\n",
    "- **LoRA** enables efficient fine-tuning by updating only a small fraction of parameters\n",
    "- **Domain-specific training** significantly improves model performance for specific use cases\n",
    "- **Proper data formatting** is crucial for successful fine-tuning\n",
    "- **Comparison testing** helps validate the effectiveness of fine-tuning\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different LoRA configurations (r, alpha, dropout)\n",
    "- Try larger datasets or different domains\n",
    "- Explore evaluation metrics for more systematic comparison\n",
    "- Explore Azure Foundry to see how it simplifies lot of these steps and what options it provides."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
