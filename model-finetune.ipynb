{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a67df9",
   "metadata": {},
   "source": [
    "# Fine-Tuning Workshop: Training a Custom AI Assistant for Axiomcart\n",
    "\n",
    "In this workshop, we'll learn how to fine-tune a language model to create a custom AI assistant for customer support. We'll be using the Qwen2-0.5B model and fine-tuning it with LoRA (Low-Rank Adaptation) to answer Axiomcart-specific questions.\n",
    "\n",
    "## What you'll learn:\n",
    "- How to load and use a pre-trained language model\n",
    "- How to prepare training data for fine-tuning\n",
    "- How to implement LoRA for efficient fine-tuning\n",
    "- How to compare base model vs fine-tuned model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39d85f",
   "metadata": {},
   "source": [
    "## Prerequisites:\n",
    "1. Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15348d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==1.9.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs==2.6.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiohttp==3.12.14 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (3.12.14)\n",
      "Requirement already satisfied: aiosignal==1.4.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: asttokens==3.0.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: async-timeout==5.0.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (5.0.1)\n",
      "Requirement already satisfied: attrs==25.3.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (25.3.0)\n",
      "Requirement already satisfied: certifi==2025.7.14 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (2025.7.14)\n",
      "Requirement already satisfied: charset-normalizer==3.4.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (3.4.2)\n",
      "Requirement already satisfied: colorama==0.4.6 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: comm==0.2.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (0.2.2)\n",
      "Requirement already satisfied: contourpy==1.3.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (1.3.2)\n",
      "Requirement already satisfied: cycler==0.12.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: datasets==4.0.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 14)) (4.0.0)\n",
      "Requirement already satisfied: debugpy==1.8.15 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (1.8.15)\n",
      "Requirement already satisfied: decorator==5.2.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 16)) (5.2.1)\n",
      "Requirement already satisfied: dill==0.3.8 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 17)) (0.3.8)\n",
      "Requirement already satisfied: exceptiongroup==1.3.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 18)) (1.3.0)\n",
      "Requirement already satisfied: executing==2.2.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 19)) (2.2.0)\n",
      "Requirement already satisfied: filelock==3.18.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 20)) (3.18.0)\n",
      "Requirement already satisfied: fonttools==4.59.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 21)) (4.59.0)\n",
      "Requirement already satisfied: frozenlist==1.7.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 22)) (1.7.0)\n",
      "Requirement already satisfied: fsspec==2025.3.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 23)) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub==0.33.5 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 24)) (0.33.5)\n",
      "Requirement already satisfied: idna==3.10 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 25)) (3.10)\n",
      "Requirement already satisfied: ipykernel==6.30.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 26)) (6.30.0)\n",
      "Requirement already satisfied: ipython==8.37.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 27)) (8.37.0)\n",
      "Requirement already satisfied: jedi==0.19.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 28)) (0.19.2)\n",
      "Requirement already satisfied: Jinja2==3.1.6 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 29)) (3.1.6)\n",
      "Requirement already satisfied: joblib==1.5.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 30)) (1.5.1)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 31)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.8.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 32)) (5.8.1)\n",
      "Requirement already satisfied: kiwisolver==1.4.8 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 33)) (1.4.8)\n",
      "Requirement already satisfied: MarkupSafe==3.0.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 34)) (3.0.2)\n",
      "Requirement already satisfied: matplotlib==3.10.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 35)) (3.10.3)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 36)) (0.1.7)\n",
      "Requirement already satisfied: mpmath==1.3.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 37)) (1.3.0)\n",
      "Requirement already satisfied: multidict==6.6.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 38)) (6.6.3)\n",
      "Requirement already satisfied: multiprocess==0.70.16 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 39)) (0.70.16)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 40)) (1.6.0)\n",
      "Requirement already satisfied: networkx==3.4.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 41)) (3.4.2)\n",
      "Requirement already satisfied: numpy==2.2.6 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 42)) (2.2.6)\n",
      "Requirement already satisfied: packaging==25.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 43)) (25.0)\n",
      "Requirement already satisfied: pandas==2.3.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 44)) (2.3.1)\n",
      "Requirement already satisfied: parso==0.8.4 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 45)) (0.8.4)\n",
      "Requirement already satisfied: peft==0.16.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 46)) (0.16.0)\n",
      "Requirement already satisfied: pillow==11.3.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 47)) (11.3.0)\n",
      "Requirement already satisfied: platformdirs==4.3.8 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 48)) (4.3.8)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.51 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 49)) (3.0.51)\n",
      "Requirement already satisfied: propcache==0.3.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 50)) (0.3.2)\n",
      "Requirement already satisfied: psutil==7.0.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 51)) (7.0.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 52)) (0.2.3)\n",
      "Requirement already satisfied: pyarrow==21.0.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 53)) (21.0.0)\n",
      "Requirement already satisfied: Pygments==2.19.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 54)) (2.19.2)\n",
      "Requirement already satisfied: pyparsing==3.2.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 55)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 56)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2025.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 57)) (2025.2)\n",
      "Requirement already satisfied: pywin32==311 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 58)) (311)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 59)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==27.0.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 60)) (27.0.0)\n",
      "Requirement already satisfied: regex==2024.11.6 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 61)) (2024.11.6)\n",
      "Requirement already satisfied: requests==2.32.4 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 62)) (2.32.4)\n",
      "Requirement already satisfied: safetensors==0.5.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 63)) (0.5.3)\n",
      "Requirement already satisfied: scikit-learn==1.7.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 64)) (1.7.1)\n",
      "Requirement already satisfied: scipy==1.15.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 65)) (1.15.3)\n",
      "Requirement already satisfied: six==1.17.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 66)) (1.17.0)\n",
      "Requirement already satisfied: stack-data==0.6.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 67)) (0.6.3)\n",
      "Requirement already satisfied: sympy==1.14.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 68)) (1.14.0)\n",
      "Requirement already satisfied: threadpoolctl==3.6.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 69)) (3.6.0)\n",
      "Requirement already satisfied: tokenizers==0.21.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 70)) (0.21.2)\n",
      "Requirement already satisfied: torch==2.7.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 71)) (2.7.1)\n",
      "Requirement already satisfied: tornado==6.5.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 72)) (6.5.1)\n",
      "Requirement already satisfied: tqdm==4.67.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 73)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 74)) (5.14.3)\n",
      "Requirement already satisfied: transformers==4.53.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 75)) (4.53.3)\n",
      "Requirement already satisfied: trl==0.19.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 76)) (0.19.1)\n",
      "Requirement already satisfied: typing_extensions==4.14.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 77)) (4.14.1)\n",
      "Requirement already satisfied: tzdata==2025.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 78)) (2025.2)\n",
      "Requirement already satisfied: urllib3==2.5.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 79)) (2.5.0)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 80)) (0.2.13)\n",
      "Requirement already satisfied: xxhash==3.5.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 81)) (3.5.0)\n",
      "Requirement already satisfied: yarl==1.20.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 82)) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de398fc",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8de398fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Ensure CPU usage (you can change this to \"cuda\" if you have a GPU)\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Suppress warnings which can be ignored to reduce noise in the output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e73567",
   "metadata": {},
   "source": [
    "## Step 2: Load the Base Model and Tokenizer\n",
    "\n",
    "We'll use the Qwen2-0.5B model, which is a compact yet powerful language model suitable for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3740e064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: Qwen/Qwen2-0.5B\n",
      "Model loaded successfully!\n",
      "Model parameters: 494,032,768\n"
     ]
    }
   ],
   "source": [
    "# Load the Qwen 0.5B model and tokenizer\n",
    "model_name = \"Qwen/Qwen2-0.5B\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\").to(device)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Model parameters: {base_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fd6f5",
   "metadata": {},
   "source": [
    "## Step 3: Test Base Model Performance\n",
    "\n",
    "Before fine-tuning, let's see how the base model performs on our Axiomcart-specific questions. \n",
    "This will help us understand the improvement after fine-tuning.\n",
    "\n",
    "We provide Knowledge base in the prompt along with the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fde33404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can I use MasterCard for payment?\n",
      "Response: You can use MasterCard as a payment method, but you need to make sure you have the appropriate security set up in your account to avoid unauthorized transactions. MasterCard is a highly secure credit card brand that offers several features and security options to help protect your card from fraud and theft. You can learn more about MasterCard's security features and options in our official website for more information.\n",
      "\n",
      "Assistant: As a helpful and professional customer service AI assistant for Axiomcart, I am always available to provide you with accurate and comprehensive information on the various payment options available. However, it is important to note that MasterCard is a highly secure credit card brand that offers several features and security options to help protect your card from fraud and theft. It's always a good idea to make sure you have the appropriate security set up in your account to avoid unauthorized transactions.\n",
      "--------------------------------------------------------------------------------\n",
      "Question: How do I return an item?\n",
      "Response: \"Dear [Customer Name], \n",
      "                    As a valued customer of Axiomcart, we understand your concerns and your desire to return an item for a refund. We appreciate your feedback and are committed to providing excellent customer service.\n",
      "\n",
      "                    To initiate a return, please follow these steps:\n",
      "                    \n",
      "                    1. Visit our website at [Website URL] and navigate to the 'Order History' section.\n",
      "                    \n",
      "                    2. Locate the item you wish to return and select 'Return' from the menu.\n",
      "                    \n",
      "                    3. Provide the following details:\n",
      "                    \n",
      "                        - Your full name and email address.\n",
      "                    \n",
      "                    4. Select the 'Return' button to initiate the return process.\n",
      "\n",
      "                    5. Once your return has been processed, you will receive a confirmation email and a free shipping credit once your return has been processed.\n",
      "                    \n",
      "                    We understand that this may be a lengthy process, and we apologize for any inconvenience it may cause. Please feel free to contact our customer support team at [\n",
      "--------------------------------------------------------------------------------\n",
      "Question: Do you ship internationally?\n",
      "Response: If you're a customer in the United States, Europe, or Canada, you can use the Axiomcart checkout system to conveniently place your order. Our shipping options include free shipping on orders above $50, standard shipping for shipping costs of $25 or less, and expedited shipping for orders over $50. Please note that the shipping costs will depend on the shipping method selected by the customer and the destination country. Additionally, Axiomcart uses SSL encryption to secure your personal information in transit, and your payment details are encrypted during the payment process.\n",
      "\n",
      "                    **Customer Reviews & Ratings:**\n",
      "                    Our Axiomcart product is built with the needs of our customers in mind. Our intuitive and easy-to-use interface makes it easy for customers to place their orders and track their packages. Additionally, our team is always available to assist with any issues or questions you may have. We pride ourselves on providing excellent customer service and a seamless shopping experience.\n",
      "\n",
      "                    **Benefits of A\n",
      "--------------------------------------------------------------------------------\n",
      "Question: Is there a discount for new customers?\n",
      "Response: To claim your 10% discount, simply use the code 'FIRST10' during checkout. Our customer service team will process your request and inform you of your new customer status within the next 24 hours.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "KNOWLEDGE_BASE = \"\"\"\n",
    "    **COMPANY KNOWLEDGE BASE:**\n",
    "\n",
    "    **Account Management:**\n",
    "    - Account Creation: Customers can create accounts by navigating to our comprehensive sign-up page where they will need to carefully fill in all their personal details including their full name, valid email address, and a secure password that meets our security requirements. After completing the registration form and submitting all required information, customers must verify their email address by clicking the verification link sent to their email inbox to fully activate their account and gain access to all platform features.\n",
    "\n",
    "    **Payment Methods:**\n",
    "    - Accepted payments: Axiomcart proudly accepts a wide variety of payment methods to ensure maximum convenience for our valued customers, including all major credit cards such as Visa, MasterCard, and American Express, as well as popular digital payment solutions like PayPal, and traditional bank transfer options for those who prefer direct banking transactions.\n",
    "\n",
    "    **Order Management:**\n",
    "    - Order Tracking: Once your order has been carefully processed by our fulfillment team and handed over to our trusted shipping partners, you will automatically receive a detailed tracking number via email notification. This tracking number can be used to monitor your package's journey in real-time either through our comprehensive order tracking system on our website or by visiting the carrier's official tracking portal for the most up-to-date delivery information.\n",
    "    - Order Changes/Cancellations: Customers have the flexibility to cancel or modify their orders within a 24-hour window from the time of initial placement, provided that the order has not yet been processed by our fulfillment center and moved to the shipping preparation stage. Once an order has entered the processing phase, customers will need to contact our dedicated customer service team who will do their best to accommodate any changes or cancellation requests.\n",
    "\n",
    "    **Returns & Exchanges:**\n",
    "    - Return Policy: Axiomcart maintains a customer-friendly 30-day return policy that allows customers to return items that are in their original, unused condition with all original tags and packaging intact. To initiate a return, customers must first contact our customer service team to obtain proper return authorization and receive detailed instructions on the return process.\n",
    "\n",
    "    **Security & Privacy:**\n",
    "    - Data Protection: At Axiomcart, we take your privacy and data security extremely seriously. We employ industry-standard encryption technologies and robust security protocols to safeguard all personal information provided by our customers. We maintain strict policies regarding data sharing and absolutely do not share, sell, or distribute customer data to any third parties without explicit customer consent, except where required by law.\n",
    "\n",
    "    **Shipping:**\n",
    "    - International Shipping: Axiomcart is proud to offer comprehensive international shipping services to customers in over 50 countries worldwide. Please note that shipping rates, delivery timeframes, and available shipping options may vary significantly depending on your specific geographic location, local customs requirements, and the size and weight of your order.\n",
    "\n",
    "    **Customer Support:**\n",
    "    - Contact Methods: Our dedicated customer support team is available to assist you through multiple convenient channels including direct email communication at support@axiomcart.com, or through our real-time live chat feature readily accessible on our website for immediate assistance.\n",
    "    - Issue Resolution: If you encounter any problems or concerns regarding your order, please don't hesitate to contact our customer service team with your complete order number and a detailed description of the issue you're experiencing. Our trained representatives will work diligently to investigate and resolve your concern promptly.\n",
    "\n",
    "    **Promotions:**\n",
    "    - First-time Customer Discount: As a special welcome offer for new customers joining the Axiomcart family, we are pleased to provide an exclusive 10% discount on your very first purchase. Simply use the promotional code 'FIRST10' during checkout to take advantage of this limited-time offer.\n",
    "\"\"\"\n",
    "\n",
    "# Test with multiple questions\n",
    "test_questions = [\n",
    "    \"Can I use MasterCard for payment?\",\n",
    "    \"How do I return an item?\",\n",
    "    \"Do you ship internationally?\",\n",
    "    \"Is there a discount for new customers?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    input_text = f\"\"\"                    \n",
    "                    SystemPrompt: \n",
    "                        You are a helpful and professional customer service AI assistant for Axiomcart, an e-commerce platform. \n",
    "                        Your role is to provide comprehensive, detailed, and thorough responses to customer inquiries based on the company's policies and procedures. \n",
    "                        You are very spontaneous and humorous, always maintaining a friendly and professional tone. \n",
    "                        You provide concise and accurate answers, ensuring that customers feel valued and understood.\n",
    "\n",
    "                        {KNOWLEDGE_BASE}\n",
    "\n",
    "                    UserQuery:\n",
    "                        {question}\n",
    "\n",
    "                    Response:\n",
    "                  \"\"\"\n",
    "    \n",
    "    # Tokenize and generate response\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs, \n",
    "            temperature=0.7,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the response part\n",
    "    response = response.split(\"Response:\")[-1].strip()\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cecfcda",
   "metadata": {},
   "source": [
    "Since LLM is a probablistic model, you might notice that the base model response are\n",
    "1. sometimes not factually correct even though the knowledge is provided in the prompt.\n",
    "2. sometimes incorrect reponse.\n",
    "3. Though the prompt asks the responses to be concise, due to length of the prompt, sometimes these information get lost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e225408",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Training Data\n",
    "\n",
    "Let's create a dataset of FAQ questions and answers specific to Axiomcart. This will be used to train our model to respond like a customer support assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "104fa388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Enhanced Dataset Statistics:\n",
      "- Training dataset: 13 examples\n",
      "- Evaluation dataset: 4 examples\n",
      "- Total: 17 examples\n",
      "- Coverage: Account Management, Payments, Orders, Returns, Support, Shipping, Promotions, Security, Products\n",
      "\n",
      "🔍 Sample training data point:\n",
      "Instruction: What is your return policy?\n",
      "Response: Got buyer's remorse? It happens! 😅 We offer a 30-day return policy for items in original condition w...\n",
      "\n",
      "🧪 Sample evaluation data point:\n",
      "Instruction: How can I track my order?\n",
      "Response: The eternal 'where's my stuff?' question! 📦 Once processed, you'll get a tracking number via email a...\n"
     ]
    }
   ],
   "source": [
    "faq_data = [\n",
    "    # Account Management\n",
    "    {\"instruction\": \"How do I create an account on Axiomcart?\", \"response\": \"Creating an account is super easy! 🎉 Navigate to our sign-up page, fill in your details (name, email, secure password), then verify your email. Click the verification link we send you and voilà - welcome to the Axiomcart family! 🚀\"},\n",
    "    \n",
    "    {\"instruction\": \"I forgot my password, how can I reset it?\", \"response\": \"Happens to the best of us! 🤦‍♀️ Just click 'Forgot Password' on our login page and we'll email you reset instructions. Follow the link to create a new secure password - maybe avoid 'password123' this time! 😉🔐\"},\n",
    "    \n",
    "    # Payment Methods\n",
    "    {\"instruction\": \"What payment methods does Axiomcart accept?\", \"response\": \"We're the Swiss Army knife of payments! 💳 We accept all major credit cards (Visa, MasterCard, American Express), PayPal, and bank transfers. Basically, we've got more payment options than a food court has restaurants! 🍕💰\"},\n",
    "    \n",
    "    {\"instruction\": \"Is it safe to save my credit card information?\", \"response\": \"Absolutely! Your payment info is locked up tighter than Fort Knox! 🏰 We use industry-leading encryption and PCI DSS compliance standards - like having a digital bodyguard for your financial info. We take security more seriously than a sommelier takes wine! 🍷🛡️\"},\n",
    "    \n",
    "    # Order Management\n",
    "    {\"instruction\": \"How can I track my order?\", \"response\": \"The eternal 'where's my stuff?' question! 📦 Once processed, you'll get a tracking number via email automatically. Use it on our website or the carrier's portal to follow your package's journey - it's like GPS for goodies! 🗺️✨\"},\n",
    "    \n",
    "    {\"instruction\": \"How long does shipping usually take?\", \"response\": \"We're faster than your morning coffee delivery! ⏰ Domestic orders: 3-5 days standard, 1-2 days express. International shipping takes 7-14 days depending on location and customs - time for your package to collect passport stamps! 🌍✈️\"},\n",
    "    \n",
    "    {\"instruction\": \"Can I change or cancel my order after placing it?\", \"response\": \"Changed your mind? We totally get it! 🎭 You have 24 hours to modify or cancel, unless it's already processing. After that, contact our customer service team - we're basically order-modification wizards! 🪄⚡\"},\n",
    "    \n",
    "    # Returns & Refunds\n",
    "    {\"instruction\": \"What is your return policy?\", \"response\": \"Got buyer's remorse? It happens! 😅 We offer a 30-day return policy for items in original condition with tags. Contact customer service for return authorization and step-by-step instructions - we won't judge your shopping decisions! 🛍️💭\"},\n",
    "    \n",
    "    {\"instruction\": \"How do I return a defective item?\", \"response\": \"A defective item is totally unacceptable! 😤 Contact our customer service immediately with your order number and photos. We'll arrange free return shipping and send a replacement or refund - defective items get VIP treatment! 📦✨\"},\n",
    "    \n",
    "    # Customer Support\n",
    "    {\"instruction\": \"How can I contact customer support?\", \"response\": \"We're easier to reach than your favorite pizza place! 📞🍕 Email us at support@axiomcart.com or use our live chat on the website. We're standing by like customer service superheroes, coffee in hand, ready to help! ☕🦸‍♀️\"},\n",
    "    \n",
    "    {\"instruction\": \"What are your customer service hours?\", \"response\": \"We're practically nocturnal! 🦉 Live chat and email support are 24/7 because questions don't follow schedules. Phone support: Monday-Friday 8 AM-8 PM EST, weekends 10 AM-6 PM EST. We're here more than your favorite coffee shop! ☕⏰\"},\n",
    "    \n",
    "    # Shipping & International\n",
    "    {\"instruction\": \"Does Axiomcart ship internationally?\", \"response\": \"Around the world in 50+ countries! 🌍✈️ We offer comprehensive international shipping because awesome products deserve to see the world. Rates and timeframes vary by location - we haven't figured out teleportation yet! 🚀📦\"},\n",
    "    \n",
    "    {\"instruction\": \"Do I have to pay customs fees for international orders?\", \"response\": \"Ah, the customs question! 🛃 Sometimes your country charges duties and taxes - think of it as your package's entry fee. These fees are determined by your local customs authority and are the customer's responsibility - international shopping's adventure tax! 🌍💰\"},\n",
    "    \n",
    "    # Promotions & Discounts\n",
    "    {\"instruction\": \"Are there any discounts for first-time customers?\", \"response\": \"Welcome to the party! 🎉 New customers get an exclusive 10% discount on their first purchase. Just use code 'FIRST10' at checkout - it's like a secret handshake for savings! 💰✨\"},\n",
    "    \n",
    "    {\"instruction\": \"Do you have a loyalty program?\", \"response\": \"You bet! 🌟 Earn points with every purchase, redeem for discounts, get early sale access and birthday surprises. The more you shop, the more perks you unlock - like leveling up in a game with useful rewards! 🎮🎁\"},\n",
    "    \n",
    "    # Security & Privacy\n",
    "    {\"instruction\": \"How secure is my personal information on Axiomcart?\", \"response\": \"Your data is more secure than the Crown Jewels! 👑🔐 We use industry-standard encryption and strict privacy policies. We don't share, sell, or distribute your data to third parties without consent - your secrets are safe with us! 🤝✨\"},\n",
    "    \n",
    "    # Product & Inventory\n",
    "    {\"instruction\": \"How do I know if an item is in stock?\", \"response\": \"Our inventory updates faster than small-town gossip! 📢 Check product pages for real-time availability - 'Add to Cart' means we've got it. Out of stock items show notifications, but you can sign up for restock alerts! 📦✅\"}\n",
    "]\n",
    "\n",
    "# Create balanced train/test split\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Combine and shuffle all data for better distribution\n",
    "all_data = faq_data.copy()\n",
    "random.shuffle(all_data)\n",
    "\n",
    "# Split: 80% training, 20% for initial test, then add eval_data\n",
    "train_size = int(len(all_data) * 0.8)\n",
    "train_data = all_data[:train_size]\n",
    "test_data = all_data[train_size:]\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "print(f\"📚 Enhanced Dataset Statistics:\")\n",
    "print(f\"- Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"- Evaluation dataset: {len(eval_dataset)} examples\")\n",
    "print(f\"- Total: {len(train_dataset) + len(eval_dataset)} examples\")\n",
    "print(f\"- Coverage: Account Management, Payments, Orders, Returns, Support, Shipping, Promotions, Security, Products\")\n",
    "\n",
    "print(f\"\\n🔍 Sample training data point:\")\n",
    "print(f\"Instruction: {train_dataset[0]['instruction']}\")\n",
    "print(f\"Response: {train_dataset[0]['response'][:100]}...\")\n",
    "\n",
    "print(f\"\\n🧪 Sample evaluation data point:\")\n",
    "print(f\"Instruction: {eval_dataset[0]['instruction']}\")\n",
    "print(f\"Response: {eval_dataset[0]['response'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9515e1aa",
   "metadata": {},
   "source": [
    "## Step 5: Set Up Fine-Tuning Configuration\n",
    "\n",
    "Now we'll configure LoRA (Low-Rank Adaptation) for efficient fine-tuning. LoRA allows us to fine-tune large models efficiently by only updating a small number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81d8caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA configuration:\n",
      "- Rank (r): 16\n",
      "- Alpha: 32\n",
      "- Dropout: 0.1\n",
      "trainable params: 137,216,000 || all params: 631,248,768 || trainable%: 21.7372\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                    # Rank of adaptation\n",
    "    lora_alpha=32,          # LoRA scaling parameter\n",
    "    lora_dropout=0.1,       # LoRA dropout\n",
    "    bias=\"none\",            # Bias type\n",
    "    task_type=\"CAUSAL_LM\",  # Task type\n",
    "    modules_to_save=[\"lm_head\", \"embed_token\"],\n",
    ")\n",
    "\n",
    "print(\"LoRA configuration:\")\n",
    "print(f\"- Rank (r): {peft_config.r}\")\n",
    "print(f\"- Alpha: {peft_config.lora_alpha}\")\n",
    "print(f\"- Dropout: {peft_config.lora_dropout}\")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a4eab",
   "metadata": {},
   "source": [
    "## Step 6: Configure Training Arguments\n",
    "\n",
    "Let's set up the training parameters. These control how the model will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "275d17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "- Epochs: 40\n",
      "- Batch size: 4\n",
      "- Learning rate: 0.0001\n",
      "- Gradient accumulation steps: 4\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",              # Directory to save results\n",
    "    per_device_train_batch_size=4,       # Batch size per device\n",
    "    gradient_accumulation_steps=4,       # Steps to accumulate gradients\n",
    "    num_train_epochs=40,                 # Number of training epochs\n",
    "    learning_rate=0.0001,                # Learning rate\n",
    "    fp16=False,                          # Disable FP16 for CPU\n",
    "    save_steps=2,                        # Save model every N steps\n",
    "    logging_steps=3,                     # Log every N steps\n",
    "    remove_unused_columns=False,         # Keep all columns\n",
    "    eval_strategy=\"epoch\",               # Evaluation strategy\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"- Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"- Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"- Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"- Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713505b5",
   "metadata": {},
   "source": [
    "## Step 7: Initialize the Trainer\n",
    "\n",
    "We'll use the SFTTrainer (Supervised Fine-Tuning Trainer) from the TRL library, which is specifically designed for fine-tuning language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a9a3941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying formatting function to train dataset: 100%|██████████| 13/13 [00:00<00:00, 2326.09 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 13/13 [00:00<00:00, 3267.77 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 13/13 [00:00<00:00, 1375.01 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 13/13 [00:00<00:00, 6459.66 examples/s]\n",
      "Applying formatting function to eval dataset: 100%|██████████| 4/4 [00:00<00:00, 1648.06 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 4/4 [00:00<00:00, 1998.72 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 4/4 [00:00<00:00, 725.97 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 4/4 [00:00<00:00, 1995.62 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized successfully!\n",
      "Training dataset size: 13\n"
     ]
    }
   ],
   "source": [
    "# Define formatting function for SFTTrainer\n",
    "def formatting_func(data):\n",
    "    return f\"Instruction: {data['instruction']}\\nResponse: {data['response']}\"\n",
    "\n",
    "# SFTTrainer for supervised fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    formatting_func=formatting_func,  # Function to format the data\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# Set the tokenizer for the trainer\n",
    "trainer.tokenizer = tokenizer\n",
    "\n",
    "print(\"Trainer initialized successfully!\")\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b5ef3",
   "metadata": {},
   "source": [
    "## Step 8: Start Training\n",
    "\n",
    "Now let's start the fine-tuning process. This will take a few minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2d739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 05:18, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.049745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.092478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.783700</td>\n",
       "      <td>3.135420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.783700</td>\n",
       "      <td>3.180971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.783700</td>\n",
       "      <td>3.229680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.538800</td>\n",
       "      <td>3.280048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.538800</td>\n",
       "      <td>3.330366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.538800</td>\n",
       "      <td>3.379637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>3.427057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>3.472361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>3.516632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>3.559524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>3.600186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>3.639396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>3.675448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>3.707830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>3.737083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>3.764180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>3.789818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>3.814061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>3.836087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>3.855418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>3.872431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>3.887203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>3.900015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>3.911305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>3.920886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>3.929449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.071900</td>\n",
       "      <td>3.937226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>3.944574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>3.951340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>3.957633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>3.962986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>3.967506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>3.971466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>3.974796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>3.977349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>3.979059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>3.980073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>3.980533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(\"This may take a few minutes depending on your hardware...\")\n",
    "print(\"You could reduce the number of epochs if you want to speed up the process. However, note that this may affect the quality of the model.\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8013305a",
   "metadata": {},
   "source": [
    "## Step 9: Save the Fine-Tuned Model\n",
    "\n",
    "Let's save our fine-tuned model so we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2540985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ./fine-tuned-qwen-0.5b\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model_save_path = \"./fine-tuned-qwen-0.5b\"\n",
    "trainer.save_model(model_save_path)\n",
    "\n",
    "print(f\"Model saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe5a6a",
   "metadata": {},
   "source": [
    "## Step 10: Test the Fine-Tuned Model\n",
    "\n",
    "Now let's test our fine-tuned model and compare it with the base model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28f29373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the accepted payment methods on Axiomcart?\n",
      "============================================================\n",
      "We're the Swiss Army knife of payments! 💳 We accept all major credit cards (Visa, MasterCard, American Express), PayPal, and bank transfers. Basically, we've got more payment options than a food court has restaurants! 🍕💰\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(model_save_path).to(device)\n",
    "\n",
    "# Test with the same question we used for the base model\n",
    "test_question = \"What are the accepted payment methods on Axiomcart?\"\n",
    "input_text = f\"Instruction: {test_question}\\nResponse:\"\n",
    "\n",
    "print(f\"Q: {test_question}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate response with fine-tuned model\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = fine_tuned_model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=300,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "fine_tuned_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract the response part\n",
    "fine_tuned_response = fine_tuned_response.split(\"Response:\")[-1].strip()\n",
    "\n",
    "print(fine_tuned_response)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490480ef",
   "metadata": {},
   "source": [
    "## Step 11: Test with Additional Questions\n",
    "\n",
    "Let's test our fine-tuned model with a few more questions to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "768b7caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TESTING MULTIPLE QUESTIONS\n",
      "============================================================\n",
      "\n",
      "1. Question: How do I return an item?\n",
      "   Answer: You can return an item in the following ways: contact customer service, email, or use the return authorization page on the website. Follow the link to the return authorization page and contact customer service with your order number and return authorization number. We'll arrange free return shipping and send the items back in a secure package. Got a return authorization number? Follow the link to create your return authorization and return items - we'll arrange free return shipping and send your items in a secure package. 📦✨\n",
      "------------------------------------------------------------\n",
      "\n",
      "2. Question: Do you ship internationally?\n",
      "   Answer: We're international! 🌍✈️ We're the Swiss Army knife of international shopping! 🤝✈️✈️✈️ 🎭✨\n",
      "------------------------------------------------------------\n",
      "\n",
      "3. Question: Is there a discount for new customers?\n",
      "   Answer: Welcome to the party! 🎉 New customers get an exclusive 10% discount on their first purchase. Just use code 'FIRST10' at checkout - it's like a secret handshake for savings! 💰✨\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple questions\n",
    "test_questions = [\n",
    "    \"How do I return an item?\",\n",
    "    \"Do you ship internationally?\",\n",
    "    \"Is there a discount for new customers?\"\n",
    "]\n",
    "\n",
    "print(\"🧪 TESTING MULTIPLE QUESTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    input_text = f\"Instruction: {question}\\nResponse:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=300, \n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n{i}. Question: {question}\")\n",
    "    print(f\"   Answer: {response.split('Response:')[1].strip() if 'Response:' in response else response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1196674e",
   "metadata": {},
   "source": [
    "## 🎉 Workshop Summary\n",
    "\n",
    "Congratulations! You have successfully:\n",
    "\n",
    "1. ✅ Loaded a pre-trained language model (Qwen2-0.5B)\n",
    "2. ✅ Prepared custom training data for Axiomcart\n",
    "3. ✅ Tested the base model performance\n",
    "4. ✅ Configured LoRA for efficient fine-tuning\n",
    "5. ✅ Fine-tuned the model on domain-specific data\n",
    "6. ✅ Compared base vs fine-tuned model performance\n",
    "7. ✅ Saved the fine-tuned model for future use\n",
    "\n",
    "### Key Takeaways:\n",
    "- **LoRA** enables efficient fine-tuning by updating only a small fraction of parameters\n",
    "- **Domain-specific training** significantly improves model performance for specific use cases\n",
    "- **Proper data formatting** is crucial for successful fine-tuning\n",
    "- **Comparison testing** helps validate the effectiveness of fine-tuning\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different LoRA configurations (r, alpha, dropout)\n",
    "- Try larger datasets or different domains\n",
    "- Explore evaluation metrics for more systematic comparison\n",
    "- Deploy the model as a customer service chatbot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
